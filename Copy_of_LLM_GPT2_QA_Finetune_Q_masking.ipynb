{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad871adcf43947649d0b2c67bb1efcd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75a761f85d12438ea2cf2856350c48cc",
              "IPY_MODEL_d74e9f47acca4f3f908f57e50193384b",
              "IPY_MODEL_33b7ec1285374477b72b3031b991e1dd"
            ],
            "layout": "IPY_MODEL_0131ccdbfd0e4477af23e4afa5d93416"
          }
        },
        "75a761f85d12438ea2cf2856350c48cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53d55958623f407b80d144b2272043c5",
            "placeholder": "​",
            "style": "IPY_MODEL_1961ecaf55434c21bab94683e8f5f783",
            "value": "Map: 100%"
          }
        },
        "d74e9f47acca4f3f908f57e50193384b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6026d9c015643e08804d744fbde7662",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9121a057ea54448eb333af5deb20ce0a",
            "value": 2
          }
        },
        "33b7ec1285374477b72b3031b991e1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b78486881824820af938cd3ddc7e395",
            "placeholder": "​",
            "style": "IPY_MODEL_3d9d2d177227487597ea33e264763876",
            "value": " 2/2 [00:00&lt;00:00, 73.18 examples/s]"
          }
        },
        "0131ccdbfd0e4477af23e4afa5d93416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d55958623f407b80d144b2272043c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1961ecaf55434c21bab94683e8f5f783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6026d9c015643e08804d744fbde7662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9121a057ea54448eb333af5deb20ce0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b78486881824820af938cd3ddc7e395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9d2d177227487597ea33e264763876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7db129729fed4eaf805da6fa9663cb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ce460d652d6481c924d589d0ae527f6",
              "IPY_MODEL_a11e5191e13445369211329095a8b73f",
              "IPY_MODEL_52bed125548f46f7aa5ef1278ab98cfe"
            ],
            "layout": "IPY_MODEL_433c8decd55948f99b2a664125473d12"
          }
        },
        "5ce460d652d6481c924d589d0ae527f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ae069caf1e441f090caf2706cc2588c",
            "placeholder": "​",
            "style": "IPY_MODEL_5e32cc68d7f743e4bad2f91d9791e84b",
            "value": "Map: 100%"
          }
        },
        "a11e5191e13445369211329095a8b73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf777485938469cb3b895aaf3568857",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce141f308b624055b67138119c5db0a0",
            "value": 2
          }
        },
        "52bed125548f46f7aa5ef1278ab98cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e18c26c38534fe59dc66d6bdee7c817",
            "placeholder": "​",
            "style": "IPY_MODEL_596e9a6737f4432ca6b7936c2c98eb70",
            "value": " 2/2 [00:00&lt;00:00, 82.51 examples/s]"
          }
        },
        "433c8decd55948f99b2a664125473d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae069caf1e441f090caf2706cc2588c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e32cc68d7f743e4bad2f91d9791e84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bf777485938469cb3b895aaf3568857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce141f308b624055b67138119c5db0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e18c26c38534fe59dc66d6bdee7c817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596e9a6737f4432ca6b7936c2c98eb70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasDhake/ColabNotebooks/blob/main/Copy_of_LLM_GPT2_QA_Finetune_Q_masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtglDM63X5tN",
        "outputId": "e6426c90-7fe6-431d-cc0f-0c687dcd1d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcYt80sfinYT",
        "outputId": "09b53459-2095-4696-a991-c001a353f8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning:"
      ],
      "metadata": {
        "id": "-Q5XeUiZmuTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best"
      ],
      "metadata": {
        "id": "z9mD_MSgiMnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Seed setting function\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 50\n",
        "set_seed(seed)\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token to tokenizer and model\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Example QA dataset\n",
        "qa_data = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
        "]\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    input_text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "    #print('input_text: ',len(input_text)) 80 and 63 character length for two samples\n",
        "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=60)\n",
        "\n",
        "    #print('inputs',len(inputs)) #Outputs 2 for both because it counts the keys in the dictionary returned by the tokenizer, not the actual number of tokens.\n",
        "    # print(\"Input text:\", input_text) # Outputs example: Input text: Question: What is the capital of France?\n",
        "                                                              #Answer: The capital of France is Paris.\n",
        "    # print(\"Tokenized input_ids:\", inputs[\"input_ids\"]) # Output example: Tokenized input_ids: [24361, 25, 1867, 318, 262, 3139, 286, 4881, 30, 198, 33706, 25, 383, 3139, 286, 4881, 318, 6342, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
        "\n",
        "    # print(\"Number of tokens:\", len(inputs[\"input_ids\"])) # 60 for both as we have set max_length=60\n",
        "\n",
        "    # Clone input_ids into labels\n",
        "    labels = inputs[\"input_ids\"].copy()\n",
        "\n",
        "    # Mask question tokens and padding tokens in labels\n",
        "    question_length = len(tokenizer(f\"Question: {example['question']}\\nAnswer:\")[\"input_ids\"]) - 1\n",
        "    #print('question_length: ',question_length) 11 then 9\n",
        "    for i in range(len(labels)):\n",
        "        if i < question_length or labels[i] == tokenizer.pad_token_id:\n",
        "            labels[i] = tokenizer.eos_token_id  # Ignore question and padding tokens\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Convert dataset to Huggingface Dataset object\n",
        "dataset = Dataset.from_list(qa_data)\n",
        "tokenized_dataset = dataset.map(preprocess_data, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "# Define collation function\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    #print('input_ids: ',input_ids.shape) #[2,60]]\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    #print('attention_mask: ',attention_mask.shape) #[2,60]\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "    #print('labels: ',labels.shape) #[2,60]\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define optimizer, criterion, and device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)  # Use -100 to ignore irrelevant tokens\n",
        "\n",
        "\n",
        "def train_model_manual_loss(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, save_dir=\"./gpt2_qa_finetuned\"):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Get the logits directly\n",
        "            #print('outputs ', outputs)\n",
        "            #print('logits ', logits.shape) #torch.Size([2, 60, 50257])\n",
        "\n",
        "            # Shift logits and labels\n",
        "            shift_logits = logits[..., :-1, :].contiguous()  # Remove the last token\n",
        "            shift_labels = labels[..., 1:].contiguous()      # Remove the first token\n",
        "            #print('shift_logits: ',shift_logits.shape) #torch.Size([2, 59, 50257])\n",
        "            #print('shift_labels: ',shift_labels.shape) #torch.Size([2, 59])\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))  # (batch_size * seq_len, vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)  # (batch_size * seq_len)\n",
        "            #print('shift_logits: ',shift_logits.shape) #torch.Size([118, 50257])\n",
        "            #print('shift_labels: ',shift_labels.shape) #torch.Size([118])\n",
        "\n",
        "            # Calculate loss using the criterion\n",
        "            loss = criterion(shift_logits, shift_labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # Shift logits and labels for validation\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "                # Reshape for loss calculation\n",
        "                shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "                shift_labels = shift_labels.view(-1)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                loss = criterion(shift_logits, shift_labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save the model if validation loss improves\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            model.save_pretrained(save_dir)\n",
        "            tokenizer.save_pretrained(save_dir)\n",
        "            print(f\"Saved best model with Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Train the model with manual loss calculation\n",
        "train_model_manual_loss(model, train_loader, val_loader, optimizer, criterion, device)\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./gpt2_qa_finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2_qa_finetuned\")\n",
        "\n",
        "# Test the model\n",
        "def generate_answer(question, model, tokenizer, device=\"cuda\"):\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    input_text = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)  # Move inputs to the same device\n",
        "    outputs = model.generate(inputs, max_length=100, num_beams=5, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer for testing\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "\n",
        "# Test generation\n",
        "question = \"What is the capital of France?\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "answer = generate_answer(question, model, tokenizer, device=device)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7db129729fed4eaf805da6fa9663cb5b",
            "5ce460d652d6481c924d589d0ae527f6",
            "a11e5191e13445369211329095a8b73f",
            "52bed125548f46f7aa5ef1278ab98cfe",
            "433c8decd55948f99b2a664125473d12",
            "0ae069caf1e441f090caf2706cc2588c",
            "5e32cc68d7f743e4bad2f91d9791e84b",
            "6bf777485938469cb3b895aaf3568857",
            "ce141f308b624055b67138119c5db0a0",
            "6e18c26c38534fe59dc66d6bdee7c817",
            "596e9a6737f4432ca6b7936c2c98eb70"
          ]
        },
        "id": "k1_fgC7WcwrP",
        "outputId": "6b64c1c7-c492-4c83-f12e-affa46655d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7db129729fed4eaf805da6fa9663cb5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 56.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.6217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.6217\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 52.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.3485\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 52.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.1929\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 51.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0797\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 46.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0299\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 51.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0126\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 51.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0056\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 51.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0026\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 45.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0014\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  9.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 1/1 [00:00<00:00, 50.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model with Validation Loss: 0.0013\n",
            "Question: What is the capital of France?\n",
            "Answer: The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference: Greedy Search"
      ],
      "metadata": {
        "id": "GDblYXZv0RPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def generate_answer(question, model, tokenizer, max_length=50, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using greedy search.\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        model (GPT2LMHeadModel): The GPT-2 model.\n",
        "        tokenizer (GPT2Tokenizer): The tokenizer.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    # Move the model to the correct device\n",
        "    model.to(device)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Prepare input\n",
        "    input_text = f\"Question: {question}\\nAnswer:\"\n",
        "    #print('input_text: ',input_text) #Question: What is the capital of france\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    #print('input_ids: ',input_ids.shape) #torch.Size([1, 12])\n",
        "\n",
        "    # Initialize the generated token IDs with the input\n",
        "    generated_ids = input_ids\n",
        "    #print('generated_ids: ',generated_ids.shape) #torch.Size([1, 12]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Forward pass to get logits for the next token\n",
        "        outputs = model(input_ids=generated_ids)\n",
        "        logits = outputs.logits\n",
        "        #print('logits: ',logits.shape) #torch.Size([1, 12, 50257] to torch.Size([1, 61, 50257]\n",
        "\n",
        "        # Select the token with the highest probability (greedy search)\n",
        "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
        "        #print('next_token_id: ',next_token_id.shape) #torch.Size([1, 1])\n",
        "\n",
        "        # Append the predicted token to the sequence\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "        #print('generated_ids: ',generated_ids) #generated_ids:  tensor([[24361,    25,  1867,   318,   262,  3139,   286,  1216,   590,   198,33706,    25,   383]], device='cuda:0')\n",
        "                                               # to\n",
        "                                               # tensor([[24361,    25,  1867,   318,   262,  3139,   286,  1216,   590,   198,\n",
        "                                                          #33706,    25,   383,  3139,   286,  1216,   590,   318,  6342,    13,\n",
        "                                                           # 383,  3139,   286,  1216,   590,   318,  6342,    13,   383,  3139,\n",
        "                                                            #286,  1216,   590,   318,  6342,    13,   383,  3139,   286,  1216,\n",
        "                                                           # 590,   318,  6342,    13,   383,  3139,   286,  1216,   590,   318,\n",
        "                                                           # 6342,    13,   383,  3139,   286,  1216,   590,   318,  6342,    13,\n",
        "                                                           # 383,  3139]], device='cuda:0')\n",
        "\n",
        "        # Stop if the end-of-sequence (EOS) token is generated\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    # Decode the generated token IDs to a text string\n",
        "    answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    #print('answer: ',answer) # Question: What is the capital of france\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "\n",
        "# Test the generate_answer function\n",
        "question = \"What is the capital of france\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "st = time.time()\n",
        "answer = generate_answer(question, model, tokenizer, device=device)\n",
        "en = time.time()\n",
        "print('total time:', en-st)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tHgfGNpxvUy",
        "outputId": "31281e99-74e5-46a2-dd90-7bcfe1312014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer:  Question: What is the capital of france\n",
            "Answer: The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital\n",
            "total time: 0.9915213584899902\n",
            "Question: What is the capital of france\n",
            "Answer: The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital of france is Paris. The capital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference: Batch Greedy Search"
      ],
      "metadata": {
        "id": "5JqZu1SO2Mk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def batch_greedy_search(questions, model, tokenizer, max_length=50, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate answers for a batch of questions using greedy search.\n",
        "    Args:\n",
        "        questions (list): A list of input questions.\n",
        "        model (GPT2LMHeadModel): The GPT-2 model.\n",
        "        tokenizer (GPT2Tokenizer): The tokenizer.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        list: The generated answers.\n",
        "    \"\"\"\n",
        "    # Move the model to the correct device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs with padding\n",
        "    input_texts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
        "    input_data = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = input_data[\"input_ids\"].to(device)\n",
        "    attention_mask = input_data[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Add padding token to handle dynamic length expansion\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    #print('pad_token_id: ',pad_token_id) # 50256\n",
        "    input_ids = torch.cat([input_ids, torch.full((input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
        "    #print('inputs_ids: ',input_ids.shape) #torch.Size([2, 13]\n",
        "    attention_mask = torch.cat([attention_mask, torch.zeros((attention_mask.size(0), 1), dtype=torch.long, device=device)], dim=1)\n",
        "    #print('attention_mask: ',attention_mask.shape) #torch.Size([2, 13])\n",
        "\n",
        "    # Track the length of each input prompt\n",
        "    input_lengths = attention_mask.sum(dim=1)  # Number of non-padding tokens per input\n",
        "    initial_input_lengths = input_lengths.clone()\n",
        "\n",
        "    batch_size = len(questions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(max_length):\n",
        "            # print(input_ids, attention_mask)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Select the token using input_lengths to avoid padding issues\n",
        "            next_token_logits = logits[torch.arange(batch_size), input_lengths - 1, :]\n",
        "            next_token_ids = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
        "            # if idx == 0 or 1:\n",
        "            #     print('b1 next_token_ids:', next_token_ids[0], next_token_ids.shape)\n",
        "            #     print('b1 next_token_ids:', next_token_ids[0][0].item(), tokenizer.decode(next_token_ids[0][0]))\n",
        "            #     print('b2 next_token_ids:', next_token_ids[1][0].item(), tokenizer.decode(next_token_ids[1][0]))\n",
        "\n",
        "            # Stop generating if EOS token is predicted for all sequences\n",
        "            eos_reached = next_token_ids == tokenizer.eos_token_id\n",
        "            if eos_reached.all():\n",
        "                break\n",
        "\n",
        "            # Append predicted tokens and update attention mask\n",
        "            input_ids.scatter_(1, input_lengths.unsqueeze(1), next_token_ids)\n",
        "            attention_mask.scatter_(1, input_lengths.unsqueeze(1), torch.ones_like(next_token_ids))\n",
        "            input_lengths += 1\n",
        "\n",
        "            # Expand input_ids and attention_mask for further predictions\n",
        "            input_ids = torch.cat([input_ids, torch.full((batch_size, 1), pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.zeros((batch_size, 1), dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "    # Extract only the generated part by slicing from input length onwards\n",
        "    answers = []\n",
        "    for i in range(batch_size):\n",
        "        generated_answer_ids = input_ids[i, initial_input_lengths[i]:]\n",
        "        print('input_ids:', input_ids[i].shape)\n",
        "        print('generated_answer_ids:', generated_answer_ids)\n",
        "        eos_idx = (generated_answer_ids == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
        "        if eos_idx.numel() > 0:\n",
        "            generated_answer_ids = generated_answer_ids[:eos_idx[0]]\n",
        "\n",
        "        answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
        "        answers.append(answer)\n",
        "\n",
        "    return answers\n",
        "\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "\n",
        "# Sample questions\n",
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who wrote Hamlet?\",\n",
        "]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "st = time.time()\n",
        "answers = batch_greedy_search(questions, model, tokenizer, device=device)\n",
        "en = time.time()\n",
        "print('Total time:', en - st)\n",
        "for q, a in zip(questions, answers):\n",
        "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
      ],
      "metadata": {
        "id": "9fRBM0ySrCEz",
        "outputId": "7261499f-ad0a-490b-b136-1a4c4b3db4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_token_id:  50256\n",
            "inputs_ids:  torch.Size([2, 13])\n",
            "attention_mask:  torch.Size([2, 13])\n",
            "input_ids: torch.Size([63])\n",
            "generated_answer_ids: tensor([  383,  3139,   286,  4881,   318,  6342,    13,   383,  3139,   286,\n",
            "         4881,   318,  6342,    13,   383,  3139,   286,  4881,   318,  6342,\n",
            "           13,   383,  3139,   286,  4881,   318,  6342,    13,   383,  3139,\n",
            "          286,  4881,   318,  6342,    13,   383,  3139,   286,  4881,   318,\n",
            "         6342,    13,   383,  3139,   286,  4881,   318,  6342,    13,   383,\n",
            "        50256], device='cuda:0')\n",
            "input_ids: torch.Size([63])\n",
            "generated_answer_ids: tensor([ 4502, 32226,  2630,  4345,  1616,    13,  4502, 32226,  2630,  4345,\n",
            "         1616,    13,  4502, 32226,  2630,  4345,  1616,    13,  4502, 32226,\n",
            "         2630,  4345,  1616,    13,  4502, 32226,  2630,  4345,  1616,    13,\n",
            "         4502, 32226,  2630,  4345,  1616,    13,  4502, 32226,  2630,  4345,\n",
            "         1616,    13,  4502, 32226,  2630,  4345,  1616,    13,  4502, 32226,\n",
            "        50256, 50256, 50256], device='cuda:0')\n",
            "Total time: 1.6071178913116455\n",
            "Q: What is the capital of France?\n",
            "A: The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The\n",
            "\n",
            "Q: Who wrote Hamlet?\n",
            "A: George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell wrote Hamlet. George Orwell\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference: Beam Search"
      ],
      "metadata": {
        "id": "vp6T8-Tm0d-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def generate_answer(question, model, tokenizer, max_length=50, num_beams=5, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using beam search for the fine-tuned GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        model (GPT2LMHeadModel): The fine-tuned model.\n",
        "        tokenizer (GPT2Tokenizer): The tokenizer.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        num_beams (int): Number of beams for beam search.\n",
        "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input text\n",
        "    input_text = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Initialize beams\n",
        "    beams = [(input_ids, attention_mask, 0.0)]  # List of (sequence, attention_mask, score)\n",
        "    completed_sequences = []\n",
        "\n",
        "    # Beam search loop\n",
        "    for _ in range(max_length):\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, mask, score in beams:\n",
        "            # Stop expanding if sequence ends with EOS token\n",
        "            if seq[0, -1] == tokenizer.eos_token_id:\n",
        "                completed_sequences.append((seq, score))  # Append sequence and score\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                outputs = model(seq, attention_mask=mask)\n",
        "                logits = outputs.logits[:, -1, :]  # Logits of the last token\n",
        "                probs = F.log_softmax(logits, dim=-1)  # Convert to log probabilities\n",
        "\n",
        "            # Get top-k tokens and their log probabilities\n",
        "            top_k_probs, top_k_tokens = torch.topk(probs, num_beams, dim=-1)\n",
        "\n",
        "            # Expand each beam\n",
        "            for prob, token in zip(top_k_probs[0], top_k_tokens[0]):\n",
        "                new_seq = torch.cat([seq, token.unsqueeze(0).unsqueeze(0)], dim=1)  # Append token\n",
        "                new_mask = torch.cat([mask, torch.ones((1, 1), device=device)], dim=1)  # Update attention mask\n",
        "                new_score = score + prob.item()  # Accumulate log probability as float\n",
        "                new_beams.append((new_seq, new_mask, new_score))\n",
        "\n",
        "        # Sort new beams by score and keep top-k\n",
        "        new_beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:num_beams]\n",
        "        beams = new_beams\n",
        "\n",
        "        # Break if all beams end with EOS token\n",
        "        if all(seq[0, -1] == tokenizer.eos_token_id for seq, _, _ in beams):\n",
        "            break\n",
        "\n",
        "    # Add remaining beams to completed sequences\n",
        "    completed_sequences.extend([(seq, float(score)) for seq, _, score in beams])\n",
        "\n",
        "    # Select the sequence with the highest score\n",
        "    best_sequence, _ = max(completed_sequences, key=lambda x: x[1])\n",
        "\n",
        "    # Decode the tokens to text\n",
        "    answer = tokenizer.decode(best_sequence[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_qa_finetuned\")\n",
        "\n",
        "# Test the generate_answer function\n",
        "question = \"What is the capital of France?\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "st = time.time()\n",
        "answer = generate_answer(question, model, tokenizer, max_length=50, num_beams=5, device=device)\n",
        "en = time.time()\n",
        "print('total time:', en-st)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "WRxYr86AyV_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b026da46-307f-4c49-a675-329ebe17f9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time: 3.1263413429260254\n",
            "Question: What is the capital of France?\n",
            "Answer: The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging:"
      ],
      "metadata": {
        "id": "7GU_uqFLv7Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Seed setting function\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 50\n",
        "set_seed(seed)\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token to tokenizer and model\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Example QA dataset\n",
        "qa_data = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
        "]\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    input_text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=60)\n",
        "\n",
        "    # Clone input_ids into labels\n",
        "    labels = inputs[\"input_ids\"].copy()\n",
        "\n",
        "    # Mask question tokens and padding tokens in labels\n",
        "    question_length = len(tokenizer(f\"Question: {example['question']}\\nAnswer:\")[\"input_ids\"]) - 1\n",
        "    for i in range(len(labels)):\n",
        "        if i < question_length or labels[i] == tokenizer.pad_token_id:\n",
        "            labels[i] = tokenizer.eos_token_id  # Ignore question and padding tokens\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Convert dataset to Huggingface Dataset object\n",
        "dataset = Dataset.from_list(qa_data)\n",
        "tokenized_dataset = dataset.map(preprocess_data, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "# Define collation function\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define optimizer, criterion, and device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)  # Use -100 to ignore irrelevant tokens\n",
        "\n",
        "\n",
        "def train_model_manual_loss(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1, save_dir=\"./gpt2_qa_finetuned\"):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            print('\\ninput_ids:', input_ids.shape,'\\ntoken ids:',input_ids, '\\ndecoded text:', tokenizer.decode(input_ids[0]))\n",
        "            print('labels:', labels.shape,'\\ntoken ids:',labels)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Get the logits directly\n",
        "\n",
        "            # Shift logits and labels\n",
        "            shift_logits = logits[..., :-1, :].contiguous()  # Remove the last token\n",
        "            shift_labels = labels[..., 1:].contiguous()      # Remove the first token\n",
        "            print('shift_labels:', shift_labels.shape,'\\ntoken ids:',shift_labels)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))  # (batch_size * seq_len, vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)  # (batch_size * seq_len)\n",
        "\n",
        "            # Calculate loss using the criterion\n",
        "            loss = criterion(shift_logits, shift_labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Train the model with manual loss calculation\n",
        "train_model_manual_loss(model, train_loader, val_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902,
          "referenced_widgets": [
            "ad871adcf43947649d0b2c67bb1efcd7",
            "75a761f85d12438ea2cf2856350c48cc",
            "d74e9f47acca4f3f908f57e50193384b",
            "33b7ec1285374477b72b3031b991e1dd",
            "0131ccdbfd0e4477af23e4afa5d93416",
            "53d55958623f407b80d144b2272043c5",
            "1961ecaf55434c21bab94683e8f5f783",
            "e6026d9c015643e08804d744fbde7662",
            "9121a057ea54448eb333af5deb20ce0a",
            "0b78486881824820af938cd3ddc7e395",
            "3d9d2d177227487597ea33e264763876"
          ]
        },
        "id": "cNHh4zWMhw6Z",
        "outputId": "4ad35b03-b48c-49c3-8c69-2b72d3a65338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad871adcf43947649d0b2c67bb1efcd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "input_ids: torch.Size([2, 60]) \n",
            "token ids: tensor([[24361,    25,  1867,   318,   262,  3139,   286,  4881,    30,   198,\n",
            "         33706,    25,   383,  3139,   286,  4881,   318,  6342,    13, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
            "        [24361,    25,  5338,  2630,   705, 28296, 30960,   198, 33706,    25,\n",
            "          4502, 32226,  2630,   705, 28296,  4458, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
            "       device='cuda:0') \n",
            "decoded text: Question: What is the capital of France?\n",
            "Answer: The capital of France is Paris.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "labels: torch.Size([2, 60]) \n",
            "token ids: tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256,    25,   383,  3139,   286,  4881,   318,  6342,    13, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    25,\n",
            "          4502, 32226,  2630,   705, 28296,  4458, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
            "       device='cuda:0')\n",
            "shift_labels: torch.Size([2, 59]) \n",
            "token ids: tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "            25,   383,  3139,   286,  4881,   318,  6342,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    25,  4502,\n",
            "         32226,  2630,   705, 28296,  4458, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
            "       device='cuda:0')\n",
            "Training Loss: 1.3734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Inf-1:\n",
        "Input- Question: What is the capital of France? \\nAnswer:\n",
        "\n",
        "Output- Question: What is the capital of France? \\nAnswer: The\n",
        "\n",
        "Inf-2:\n",
        "Input- Question: What is the capital of France? \\nAnswer: The\n",
        "\n",
        "Output- Question: What is the capital of France? \\nAnswer: The capital"
      ],
      "metadata": {
        "id": "vPVkRqN9xaul",
        "outputId": "f0d90978-bc93-4bca-ea68-d99efdb68074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-8-35fb409fe1b2>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-35fb409fe1b2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Inf-1:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Input- Question: What is the capital of France? \\nAnswer:\n",
        "\n"
      ],
      "metadata": {
        "id": "VrRzJvHZucDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}